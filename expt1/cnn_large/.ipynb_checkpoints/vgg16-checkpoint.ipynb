{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-04-20 20:55:12--  https://www.dropbox.com/s/blrajqirr1p31v0/cifar10_nin.caffemodel\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.82.1\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.82.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://dl.dropboxusercontent.com/content_link/5ocsf1uCJCEV7SlQXMCfzEodyPKk9AQU7EA3BH3Kj2GQ0ORcknktIuxHgkfBitOE/file [following]\n",
      "--2017-04-20 20:55:12--  https://dl.dropboxusercontent.com/content_link/5ocsf1uCJCEV7SlQXMCfzEodyPKk9AQU7EA3BH3Kj2GQ0ORcknktIuxHgkfBitOE/file\n",
      "Resolving dl.dropboxusercontent.com (dl.dropboxusercontent.com)... 162.125.82.6\n",
      "Connecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|162.125.82.6|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3869548 (3.7M) [application/octet-stream]\n",
      "Saving to: ‘cifar10_nin.caffemodel’\n",
      "\n",
      "100%[======================================>] 3,869,548    674KB/s   in 6.9s   \n",
      "\n",
      "2017-04-20 20:55:20 (551 KB/s) - ‘cifar10_nin.caffemodel’ saved [3869548/3869548]\n",
      "\n",
      "--2017-04-20 20:55:20--  https://gist.githubusercontent.com/ebenolson/91e2cfa51fdb58782c26/raw/b015b7403d87b21c6d2e00b7ec4c0880bbeb1f7e/model.prototxt\n",
      "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 151.101.76.133\n",
      "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|151.101.76.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4550 (4.4K) [text/plain]\n",
      "Saving to: ‘model.prototxt’\n",
      "\n",
      "100%[======================================>] 4,550       --.-K/s   in 0.002s  \n",
      "\n",
      "2017-04-20 20:55:20 (2.25 MB/s) - ‘model.prototxt’ saved [4550/4550]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.dropbox.com/s/blrajqirr1p31v0/cifar10_nin.caffemodel \n",
    "!wget https://gist.githubusercontent.com/ebenolson/91e2cfa51fdb58782c26/raw/b015b7403d87b21c6d2e00b7ec4c0880bbeb1f7e/model.prototxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: GeForce GTX 980 Ti (CNMeM is disabled, cuDNN 5110)\n"
     ]
    }
   ],
   "source": [
    "import caffe\n",
    "\n",
    "net_caffe = caffe.Net('model.prototxt', 'cifar10_nin.caffemodel', caffe.TEST)\n",
    "\n",
    "import lasagne\n",
    "from lasagne.layers import InputLayer, DropoutLayer, FlattenLayer\n",
    "from lasagne.layers.dnn import Conv2DDNNLayer as ConvLayer\n",
    "from lasagne.layers import Pool2DLayer as PoolLayer\n",
    "from lasagne.utils import floatX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = {}\n",
    "net['input'] = InputLayer((None, 3, 32, 32))\n",
    "net['conv1'] = ConvLayer(net['input'], num_filters=192, filter_size=5, pad=2, flip_filters=False)\n",
    "net['cccp1'] = ConvLayer(net['conv1'], num_filters=160, filter_size=1, flip_filters=False)\n",
    "net['cccp2'] = ConvLayer(net['cccp1'], num_filters=96, filter_size=1, flip_filters=False)\n",
    "net['pool1'] = PoolLayer(net['cccp2'], pool_size=3, stride=2, mode='max', ignore_border=False)\n",
    "net['drop3'] = DropoutLayer(net['pool1'], p=0.5)\n",
    "net['conv2'] = ConvLayer(net['drop3'], num_filters=192, filter_size=5, pad=2, flip_filters=False)\n",
    "net['cccp3'] = ConvLayer(net['conv2'], num_filters=192, filter_size=1, flip_filters=False)\n",
    "net['cccp4'] = ConvLayer(net['cccp3'], num_filters=192, filter_size=1, flip_filters=False)\n",
    "net['pool2'] = PoolLayer(net['cccp4'], pool_size=3, stride=2, mode='average_exc_pad', ignore_border=False)\n",
    "net['drop6'] = DropoutLayer(net['pool2'], p=0.5)\n",
    "net['conv3'] = ConvLayer(net['drop6'], num_filters=192, filter_size=3, pad=1, flip_filters=False)\n",
    "net['cccp5'] = ConvLayer(net['conv3'], num_filters=192, filter_size=1, flip_filters=False)\n",
    "net['cccp6'] = ConvLayer(net['cccp5'], num_filters=10, filter_size=1, flip_filters=False)\n",
    "net['pool3'] = PoolLayer(net['cccp6'], pool_size=8, mode='average_exc_pad', ignore_border=False)\n",
    "net['output'] = lasagne.layers.FlattenLayer(net['pool3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "layers_caffe = dict(zip(list(net_caffe._layer_names), net_caffe.layers))\n",
    "\n",
    "for name, layer in net.items():\n",
    "    try:\n",
    "        layer.W.set_value(layers_caffe[name].blobs[0].data)\n",
    "        layer.b.set_value(layers_caffe[name].blobs[1].data)       \n",
    "    except AttributeError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cccp1': <caffe._caffe.Layer at 0x7fb69a0a2320>,\n",
       " 'cccp2': <caffe._caffe.Layer at 0x7fb69a0a21b8>,\n",
       " 'cccp3': <caffe._caffe.Layer at 0x7fb69a0a22a8>,\n",
       " 'cccp4': <caffe._caffe.Layer at 0x7fb69a0a2398>,\n",
       " 'cccp5': <caffe._caffe.Layer at 0x7fb69a0a2848>,\n",
       " 'cccp6': <caffe._caffe.Layer at 0x7fb69a0a2938>,\n",
       " 'conv1': <caffe._caffe.Layer at 0x7fb69a0a4d70>,\n",
       " 'conv2': <caffe._caffe.Layer at 0x7fb69a0a2140>,\n",
       " 'conv3': <caffe._caffe.Layer at 0x7fb69a0a2758>,\n",
       " 'drop3': <caffe._caffe.Layer at 0x7fb69a0a2410>,\n",
       " 'drop6': <caffe._caffe.Layer at 0x7fb69a0a26e0>,\n",
       " 'input': <caffe._caffe.Layer at 0x7fb69a0a4c80>,\n",
       " 'poo1': <caffe._caffe.Layer at 0x7fb69a0a2500>,\n",
       " 'pool2': <caffe._caffe.Layer at 0x7fb69a0a2668>,\n",
       " 'pool3': <caffe._caffe.Layer at 0x7fb69a0a2a28>,\n",
       " 'relu1': <caffe._caffe.Layer at 0x7fb69a0a4b18>,\n",
       " 'relu2': <caffe._caffe.Layer at 0x7fb69a0a2050>,\n",
       " 'relu3': <caffe._caffe.Layer at 0x7fb69a0a27d0>,\n",
       " 'relu_cccp1': <caffe._caffe.Layer at 0x7fb69a0a2230>,\n",
       " 'relu_cccp2': <caffe._caffe.Layer at 0x7fb69a0a2578>,\n",
       " 'relu_cccp3': <caffe._caffe.Layer at 0x7fb69a0a2488>,\n",
       " 'relu_cccp4': <caffe._caffe.Layer at 0x7fb69a0a20c8>,\n",
       " 'relu_cccp5': <caffe._caffe.Layer at 0x7fb69a0a28c0>,\n",
       " 'relu_cccp6': <caffe._caffe.Layer at 0x7fb69a0a29b0>}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers_caffe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      "conv1\n",
      "relu1\n",
      "cccp1\n",
      "relu_cccp1\n",
      "cccp2\n",
      "relu_cccp2\n",
      "poo1\n",
      "drop3\n",
      "conv2\n",
      "relu2\n",
      "cccp3\n",
      "relu_cccp3\n",
      "cccp4\n",
      "relu_cccp4\n",
      "pool2\n",
      "drop6\n",
      "conv3\n",
      "relu3\n",
      "cccp5\n",
      "relu_cccp5\n",
      "cccp6\n",
      "relu_cccp6\n",
      "pool3\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(net_caffe._layer_names)):\n",
    "    print net_caffe._layer_names[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      "conv1_1\n",
      "relu1_1\n",
      "conv1_2\n",
      "relu1_2\n",
      "pool1\n",
      "conv2_1\n",
      "relu2_1\n",
      "conv2_2\n",
      "relu2_2\n",
      "pool2\n",
      "conv3_1\n",
      "relu3_1\n",
      "conv3_2\n",
      "relu3_2\n",
      "conv3_3\n",
      "relu3_3\n",
      "pool3\n",
      "conv4_1\n",
      "relu4_1\n",
      "conv4_2\n",
      "relu4_2\n",
      "conv4_3\n",
      "relu4_3\n",
      "pool4\n",
      "conv5_1\n",
      "relu5_1\n",
      "conv5_2\n",
      "relu5_2\n",
      "conv5_3\n",
      "relu5_3\n",
      "pool5\n",
      "fc6\n",
      "relu6\n",
      "drop6\n",
      "fc7\n",
      "relu7\n",
      "drop7\n",
      "fc8\n",
      "prob\n"
     ]
    }
   ],
   "source": [
    "net_caffe1 = caffe.Net('VGG_ILSVRC_16_layers_deploy.prototxt', 'vgg16_dsd.caffemodel', caffe.TEST)\n",
    "for i in range(len(net_caffe1._layer_names)):\n",
    "    print net_caffe1._layer_names[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lasagne.layers import InputLayer\n",
    "from lasagne.layers import DenseLayer\n",
    "from lasagne.layers import NonlinearityLayer\n",
    "from lasagne.layers import DropoutLayer\n",
    "from lasagne.layers import Pool2DLayer as PoolLayer\n",
    "from lasagne.layers.dnn import Conv2DDNNLayer as ConvLayer\n",
    "from lasagne.nonlinearities import softmax\n",
    "\n",
    "net={}\n",
    "net['input'] = InputLayer((None, 3, 224, 224))\n",
    "net['conv1_1'] = ConvLayer(net['input'], 64, 3, pad=1, flip_filters=False)\n",
    "net['conv1_2'] = ConvLayer(net['conv1_1'], 64, 3, pad=1, flip_filters=False)\n",
    "net['pool1'] = PoolLayer(net['conv1_2'], 2)\n",
    "net['conv2_1'] = ConvLayer(net['pool1'], 128, 3, pad=1, flip_filters=False)\n",
    "net['conv2_2'] = ConvLayer(net['conv2_1'], 128, 3, pad=1, flip_filters=False)\n",
    "net['pool2'] = PoolLayer(net['conv2_2'], 2)\n",
    "net['conv3_1'] = ConvLayer(net['pool2'], 256, 3, pad=1, flip_filters=False)\n",
    "net['conv3_2'] = ConvLayer(net['conv3_1'], 256, 3, pad=1, flip_filters=False)\n",
    "net['conv3_3'] = ConvLayer(net['conv3_2'], 256, 3, pad=1, flip_filters=False)\n",
    "net['pool3'] = PoolLayer(net['conv3_3'], 2)\n",
    "net['conv4_1'] = ConvLayer(net['pool3'], 512, 3, pad=1, flip_filters=False)\n",
    "net['conv4_2'] = ConvLayer(net['conv4_1'], 512, 3, pad=1, flip_filters=False)\n",
    "net['conv4_3'] = ConvLayer(net['conv4_2'], 512, 3, pad=1, flip_filters=False)\n",
    "net['pool4'] = PoolLayer(net['conv4_3'], 2)\n",
    "net['conv5_1'] = ConvLayer(net['pool4'], 512, 3, pad=1, flip_filters=False)\n",
    "net['conv5_2'] = ConvLayer(net['conv5_1'], 512, 3, pad=1, flip_filters=False)\n",
    "net['conv5_3'] = ConvLayer(net['conv5_2'], 512, 3, pad=1, flip_filters=False)\n",
    "net['pool5'] = PoolLayer(net['conv5_3'], 2)\n",
    "net['fc6'] = DenseLayer(net['pool5'], num_units=4096)\n",
    "net['fc6_dropout'] = DropoutLayer(net['fc6'], p=0.5)\n",
    "net['fc7'] = DenseLayer(net['fc6_dropout'], num_units=4096)\n",
    "net['fc7_dropout'] = DropoutLayer(net['fc7'], p=0.5)\n",
    "net['fc8'] = DenseLayer(net['fc7_dropout'], num_units=1000, nonlinearity=None)\n",
    "net['prob'] = NonlinearityLayer(net['fc8'], softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layers_caffe = dict(zip(list(net_caffe1._layer_names), net_caffe1.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for name, layer in net.items():\n",
    "    try:\n",
    "        layer.W.set_value(layers_caffe[name].blobs[0].data)\n",
    "        layer.b.set_value(layers_caffe[name].blobs[1].data)       \n",
    "    except AttributeError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def degree_plot_fc(data,name):\n",
    "    ## load weight matrix\n",
    "    # data = np.loadtxt('512/weight_0.txt')  # load all the edges, which is saved in facebook_combined.txt file\n",
    "    # normalize the data to [0,1]\n",
    "    data = np.abs(data)\n",
    "    data_normalized = (data - data.min())/data.ptp()\n",
    "    # data_normalized\n",
    "\n",
    "    vec_data = data_normalized.flatten()\n",
    "    a = int(0.99*data.size)\n",
    "    print a\n",
    "    thres = np.sort(vec_data)[a]\n",
    "    print thres\n",
    "\n",
    "    # binarize the weight matrix using a threshold\n",
    "    # thres = 0.35\n",
    "    data_normalized[data_normalized > thres] = 1\n",
    "    data_normalized[data_normalized<=thres] = 0\n",
    "\n",
    "    # create edge list\n",
    "    edge_list= []\n",
    "    data_normalized = data_normalized.astype(int)\n",
    "    # create edge list\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "            if data_normalized[i,j] == 1:\n",
    "                edge_list.append((i,j+data.shape[0]))\n",
    "\n",
    "    # create the graph\n",
    "    G = nx.Graph(edge_list)  # generate the facebook graph\n",
    "\n",
    "    ################### if use all the nodes in one layer###################\n",
    "    degree_seq = nx.degree(G).values()\n",
    "    ################### if use only the target nodes in one layer###################\n",
    "    # nx.degree(G).items()[0][1]\n",
    "    # dest = filter(lambda x: x[0] > data.shape[0], nx.degree(G).items())\n",
    "    # degree_seq = sorted([j for i,j in dest])\n",
    "\n",
    "\n",
    "    degree_sequence = sorted(degree_seq, reverse=True)\n",
    "    bins = np.arange(min(degree_sequence)-0.5, max(degree_sequence)+1, 1)\n",
    "    h, bins = np.histogram(degree_sequence, density=True, bins=bins)\n",
    "\n",
    "    counts = h*len(degree_sequence)\n",
    "    p = plt.figure(figsize=(6,4), dpi=80)\n",
    "    p3 = p.add_subplot(111)\n",
    "    p3.set_xscale(\"log\")\n",
    "    p3.set_yscale(\"log\")\n",
    "    p3.set_xlim(1e0, 1e4) \n",
    "    p3.set_ylim(1e0, 1e4) \n",
    "    p3.set_xlabel('degree')\n",
    "    p3.set_ylabel('counts')\n",
    "    p3.set_title(\"log-log plot\")\n",
    "    p3.plot(np.arange(len(counts)), counts, \"o-\")\n",
    "    plt.tight_layout()\n",
    "    # plt.subplots_adjust(top=0.9)\n",
    "    p.savefig('imagenet_vgg16_{0}.png'.format(name))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def degree_plot_conv(data,name):\n",
    "    ## load weight matrix\n",
    "    # data = np.loadtxt('512/weight_0.txt')  # load all the edges, which is saved in facebook_combined.txt file\n",
    "    # normalize the data to [0,1]\n",
    "    data = np.abs(data)\n",
    "    data_normalized = (data - data.min())/data.ptp()\n",
    "    # data_normalized\n",
    "    print data_normalized.max(), data_normalized.min()\n",
    "\n",
    "    vec_data = data_normalized.flatten()\n",
    "    a = int(0.8*data.size)\n",
    "    print a\n",
    "    thres = np.sort(vec_data)[a]\n",
    "    print thres\n",
    "\n",
    "    # binarize the weight matrix using a threshold\n",
    "    # thres = 0.35\n",
    "    data_normalized[data_normalized > thres] = 1\n",
    "    data_normalized[data_normalized<=thres] = 0\n",
    "\n",
    "\n",
    "\n",
    "    # count the degree of each neuron \n",
    "    degree =  np.sum(data_normalized,axis=(1,2,3)).astype('int')\n",
    "#     print degree_seq.shape\n",
    "#     print degree_seq\n",
    "    degree_seq = degree.tolist()\n",
    "    print degree_seq\n",
    "\n",
    "\n",
    "\n",
    "    degree_sequence = sorted(degree_seq, reverse=True)\n",
    "    bins = np.arange(min(degree_sequence)-0.5, max(degree_sequence)+1, 1)\n",
    "    h, bins = np.histogram(degree_sequence, density=True, bins=bins)\n",
    "\n",
    "    counts = h*len(degree_sequence)\n",
    "    p = plt.figure(figsize=(6,4), dpi=80)\n",
    "    p3 = p.add_subplot(111)\n",
    "    p3.set_xscale(\"log\")\n",
    "    p3.set_yscale(\"log\")\n",
    "    p3.set_xlim(1e0, 1e4) \n",
    "    p3.set_ylim(1e0, 1e4) \n",
    "    p3.set_xlabel('degree')\n",
    "    p3.set_ylabel('counts')\n",
    "    p3.set_title(\"log-log plot\")\n",
    "    p3.plot(np.arange(len(counts)), counts, \"o-\")\n",
    "    plt.tight_layout()\n",
    "    # plt.subplots_adjust(top=0.9)\n",
    "    p.savefig('imagenet_vgg16_{0}.png'.format(name))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 3, 3, 3)\n",
      "(64,)\n",
      "(64, 64, 3, 3)\n",
      "(64,)\n",
      "(128, 64, 3, 3)\n",
      "(128,)\n",
      "(128, 128, 3, 3)\n",
      "(128,)\n",
      "(256, 128, 3, 3)\n",
      "(256,)\n",
      "(256, 256, 3, 3)\n",
      "(256,)\n",
      "(256, 256, 3, 3)\n",
      "(256,)\n",
      "(512, 256, 3, 3)\n",
      "(512,)\n",
      "(512, 512, 3, 3)\n",
      "(512,)\n",
      "(512, 512, 3, 3)\n",
      "(512,)\n",
      "(512, 512, 3, 3)\n",
      "(512,)\n",
      "(512, 512, 3, 3)\n",
      "(512,)\n",
      "(512, 512, 3, 3)\n",
      "(512,)\n",
      "(4096, 25088)\n",
      "(4096,)\n",
      "(4096, 4096)\n",
      "(4096,)\n",
      "(1000, 4096)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "a = lasagne.layers.get_all_param_values(net['prob'])\n",
    "name='conv1'\n",
    "# degree_plot(a, name)\n",
    "for i,value in enumerate(a):\n",
    "    print value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 3, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-bfccdae1b3ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdegree_plot_conv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'fc1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mnames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'conv1'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'conv2'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'conv3'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'conv4'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'conv5'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fc1'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'fc2'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fc3'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# convolutional layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-5c789b10ad49>\u001b[0m in \u001b[0;36mdegree_plot_conv\u001b[1;34m(data, name)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m# data = np.loadtxt('512/weight_0.txt')  # load all the edges, which is saved in facebook_combined.txt file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m# normalize the data to [0,1]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mdata_normalized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mptp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# data_normalized\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: global name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "degree_plot_conv(a[0],'fc1')\n",
    "# names = ['conv1','conv2','conv3','conv4', 'conv5', 'fc1','fc2', 'fc3']\n",
    "# convolutional layers\n",
    "\n",
    "\n",
    "# fully connected layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
